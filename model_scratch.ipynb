{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245b372e",
   "metadata": {},
   "source": [
    "# Categorical Naive Bayes Classifier \n",
    "\n",
    "Naive bayes models are a broad family of supervised machine learning models that can be used for both classification and regression tasks. They are all based on Bayes' theorem with the so-called _\"naive assumption\"_ which assumes that all features are indepedent from each other. Specifically, in this notebook we will implement a categorical naive Bayes classifier which assumes that the features can only take on discrete values. These values do not represent a count, instead they each discrete value is associated with a category or label. A typical use for this type of model is in the detection of spam emails. Below is data collected on old spam SMS texts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd4dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>to</th>\n",
       "      <th>free</th>\n",
       "      <th>txt</th>\n",
       "      <th>your</th>\n",
       "      <th>or</th>\n",
       "      <th>now</th>\n",
       "      <th>mobile</th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>å2000</th>\n",
       "      <th>mins</th>\n",
       "      <th>landline</th>\n",
       "      <th>shows</th>\n",
       "      <th>camera</th>\n",
       "      <th>16</th>\n",
       "      <th>box</th>\n",
       "      <th>only</th>\n",
       "      <th>holiday</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      call  to  free  txt  your  or  now  mobile  claim  text  ...  å2000  \\\n",
       "0        0   0     0    0     0   1    0       0      0     0  ...      0   \n",
       "1        0   0     0    0     0   0    0       0      0     0  ...      0   \n",
       "2        0   1     0    1     0   0    0       0      0     0  ...      0   \n",
       "3        0   0     0    0     0   1    0       0      0     0  ...      0   \n",
       "4        0   1     0    0     0   0    0       0      0     0  ...      0   \n",
       "...    ...  ..   ...  ...   ...  ..  ...     ...    ...   ...  ...    ...   \n",
       "5567     1   0     0    0     0   0    0       0      1     0  ...      0   \n",
       "5568     0   1     0    0     0   0    0       0      0     0  ...      0   \n",
       "5569     0   0     0    0     0   1    0       0      0     0  ...      0   \n",
       "5570     0   1     1    0     0   1    0       0      0     0  ...      0   \n",
       "5571     0   1     0    0     0   0    0       0      0     0  ...      0   \n",
       "\n",
       "      mins  landline  shows  camera  16  box  only  holiday  target  \n",
       "0        0         0      0       0   0    0     1        0       0  \n",
       "1        0         0      0       0   0    0     0        0       0  \n",
       "2        0         0      0       0   0    0     0        0       1  \n",
       "3        0         0      0       0   0    0     0        0       0  \n",
       "4        0         0      0       0   0    0     0        0       0  \n",
       "...    ...       ...    ...     ...  ..  ...   ...      ...     ...  \n",
       "5567     0         0      0       0   0    0     0        0       1  \n",
       "5568     0         0      0       0   0    0     0        0       0  \n",
       "5569     0         0      0       0   0    0     0        0       0  \n",
       "5570     0         0      0       0   0    0     0        0       0  \n",
       "5571     0         0      0       0   0    0     0        0       0  \n",
       "\n",
       "[5572 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports,\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Loading data,\n",
    "data = np.load(\"dataset.npz\", allow_pickle=True)\n",
    "X, y, feature_labels = data[\"features\"], data[\"targets\"], data[\"feature_labels\"]\n",
    "\n",
    "# Creating dataframe,\n",
    "df = pd.DataFrame(data=X, columns=feature_labels)\n",
    "df[\"target\"] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ce9b8",
   "metadata": {},
   "source": [
    "Each feature \"call\", \"to\", \"free\" are words that are that may or may not be in an SMS text message. Therefore, each feature $x_j$ can only have a value of 0 (the word is not in the text) or 1 (the word appears in the text atleast once). Our features are encoded in a feature vector $X = [x_1, x_2, x_3, ..., x_M]$ with $M=50$ number of features and $C=2$ number of classes such that $y=0$ for non-spam and $y=1$ for spam texts. Our starting point for a naive Bayes model is Bayes' theorem which states that, \n",
    "\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Essentially, we can find the probability of an event $A$ occuring given that $B$ has occured (posterior probability) based on known information such as the likelihood $P(B|A)$, prior $P(A)$ and marginal $P(B)$ probabilities. In our machine learning context, we let $B$ represent the event that the feature vector $X$ is observed and event $A$ that the class label $y$ is the associated classification. \n",
    "\n",
    "$$P(y|X) = \\frac{P(y)P(X|y)}{P(X)}$$\n",
    "\n",
    "The event that the feature vector $X$ is observed can be thought of as the independent events of $x_1$, $x_2$, $x_3$, ..., $x_M$ being observed simulataneously. This assumption is why we call the model is called a **naive** Bayes model. With this, we are able to write,\n",
    "\n",
    "$$P(y|X) = \\frac{P(y)}{P(X)} \\prod_{j=1}^{M} P(x_j|y) $$\n",
    "\n",
    "from the likelihood. We may also rewrite the the marginal probability $P(X)$ using an identity from probability theory,\n",
    "\n",
    "$$P(y|X) = \\frac{P(y)\\prod_{j=1}^{M} P(x_j|y)}{\\sum_{k=1}^{C} P(X|y_k)P(y_k)}$$\n",
    "\n",
    "Once again, using the _\"naive assumption\"_,\n",
    "\n",
    "$$P(y|X) = \\frac{P(y)\\prod_{j=1}^{M} P(x_j|y)}{\\sum_{k=1}^{C} P(y_k) \\prod_{j=1}^{M} P(x_j|y_k)}$$\n",
    "\n",
    "Notice that the margin probabilitiy, our denominator, is simply a normalisation factor since it runs over all possible class labels. Let us call this normalisation factor $Z = \\sum_{k=1}^{C} P(y_k) \\prod_{j=1}^{M} P(x_j|y_k)$. The probability of observing the class label $y=y_l$ given an associated feature vector $X$ has been measured is,\n",
    "\n",
    "$$P(y=y_l|X) = \\frac{1}{Z}P(y=y_l)\\prod_{j=1}^{M} P(x_j|y=y_k)$$\n",
    "\n",
    "This formula is basically the entire model summarised into a single equation. When making a prediction $\\hat{y}$, the model computes $P(y|X)$ (or a another quantity proportional to it) for all class labels $y_1$, $y_2$, ..., $y_C$. The class label with the largest probability is assigned as the predicted class such that, \n",
    "\n",
    "$$\\hat{y} = \\text{argmax}_{y} \\left[ P(y|X) \\right]$$\n",
    "\n",
    "Since we are dealing with discrete features, computing Z is quite straight-foward. For $P(y=y_k)$ we simply have $P(y=y_k)=N_k/N$ where $N_k$ is the nymber of examples with the label $y=y_l$ and $N$ is the total number of examples. Calculating $P(x_j|y_k)$ is a little more complicated,\n",
    "\n",
    "$$P(x_j = \\nu|y_k) = \\frac{N_{\\nu j k} + \\alpha}{N_k + n_j \\alpha}$$\n",
    "\n",
    "In which, $N_{\\nu j k}$ is the number of samples with the class label $y=y_k$ and the feature value $x_j = \\nu$ (for some $j = 1, 2, 3, ..., M$), $n_j$ is the number of features in $X$ and $\\alpha$ with is the Laplace smoothing parameter (usually $0< \\alpha \\leq 1$). The smoothing parameter $\\alpha$ ensures that we do not encounter zero probabilites when $N_{\\nu j k}=0$ which breaks our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f9bd0",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Below, we implement a very basic MNB. The essential idea is that we want to reserve the majority of the computation for when we create and fit the model rather than when we need to calculate predictios. This will save us time and computational resources. We want our model calculate $P(y|X)$ for all possible labels $y$ given some observed $X$. Let the all $P(y|X)$ be stored in some array $\\mathbf{P}$ such that,\n",
    "\n",
    " $\\mathbf{P} = \\left[P(y=y_1|X), P(y=y_2|X), ..., P(y=y_C|X) \\right]$  \n",
    "\n",
    "We also define the following arrays to store the likelihood and prior probabilities, \n",
    "\n",
    "$\\mathbf{P_{Xy}} = \\left[P(X|y=y_1), P(X|y=y_1), ..., P(X|y=y_C) \\right]$\n",
    "\n",
    "$\\mathbf{P_{y}} = \\left[P(y=y_1), P(y=y_2), ..., P(y=y_3) \\right]$\n",
    "\n",
    "With this formulation, our probabilites are calculated via,\n",
    "\n",
    "$\\mathbf{P} = \\frac{\\mathbf{P_{y}} \\mathbf{P_{Xy}}}{ \\mathbf{P_{y}} \\cdot \\mathbf{P_{Xy}}}$\n",
    "\n",
    "In which, $Z = \\mathbf{P_{y}} \\cdot \\mathbf{P_{Xy}}$. In the model fitting, we create a so-called likelihood tensor (3D array) whose elements are $P(x_j|y)$. Let this tensor be denoted by $\\mathbf{T}$ with the elements $T_{kj \\nu} = P(x_j=\\nu|y=y_k)$. From $\\mathbf{T}$, we are able to simply pick out its elements and reconstruct $\\mathbf{P_{Xy}}$ and $\\mathbf{P_{y}}$ to compute the probabilities $\\mathbf{P}$. Note that $\\mathbf{T}$ has the dimensions $[C, M, n_j]$ where $n_j$ is the number of unique values a feature $x_j$ could have ($n_j=2$ for all $j$ in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c80bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNaiveBayes():\n",
    "    \"\"\"\n",
    "    A simple implementation of the Categorical Naive Bayes classifier for discrete/categorical feature data.\n",
    "\n",
    "    This model assumes that each feature follows a multinomial distribution conditioned on the class label.\n",
    "    It applies Laplace smoothing controlled by the hyperparameter `alpha`.\n",
    "\n",
    "    Attributes:\n",
    "        X (ndarray): The training feature matrix of shape (n_samples, n_features).\n",
    "        y (ndarray): The target labels corresponding to `X`, of shape (n_samples,).\n",
    "        alpha (float): Laplace smoothing parameter (must be >= 0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"Constructor method. Class variables are stored or computed.\"\"\"\n",
    "\n",
    "        # Creating class variables,\n",
    "        self.X, self.y = None, None\n",
    "        self.nX_possible_values, self.y_possible_values = None, None\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Placeholders,\n",
    "        self.Py_vector = None\n",
    "        self.likelihoods = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Bulk of the required calculation is performed by fitting the model. Specifically, our probability vectors are \n",
    "        computed.\"\"\"\n",
    "\n",
    "        # Assigning class variables,\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "        # Computing possible values for features and classes,\n",
    "        self.nX_possible_values = np.apply_along_axis(lambda col: len(np.unique(col)), axis=0, arr=X)\n",
    "        self.n_features = X.shape[1]\n",
    "        self.feature_values = [np.unique(X[:, i]) for i in range(self.n_features)]\n",
    "\n",
    "        # Computing P(y) for all class labels,\n",
    "        self.y_possible_values, y_counts = np.unique(y, return_counts=True)\n",
    "        self.Py_vector = y_counts/len(y) # <-- Formula \n",
    "        self.n_classes = len(self.y_possible_values)\n",
    "        self.max_categories = max(self.nX_possible_values)\n",
    "\n",
    "        # Creating likelihood tensor,\n",
    "        self.likelihoods = np.full((self.n_classes, self.n_features, self.max_categories), fill_value=1, dtype=float)\n",
    "\n",
    "        # Computing all P(x_i|y=yk) for lik,\n",
    "        for cls_idx, cls in enumerate(self.y_possible_values):\n",
    "\n",
    "            cls_idxs = np.where(self.y==cls)[0]\n",
    "            X_given_y = X[cls_idxs]\n",
    "\n",
    "            # Double loop over features and then possible feature values,\n",
    "            for feature_idx in range(self.n_features):\n",
    "\n",
    "                # P(x_i|y=yk) for all values x_i can take,\n",
    "                for feature_value in self.feature_values[feature_idx]:\n",
    "                    n_instances = len(np.where(X_given_y.T[feature_idx]==feature_value)[0])\n",
    "                    n_j = self.nX_possible_values[feature_idx]\n",
    "                    Pxiy = (n_instances + self.alpha)/(len(cls_idxs) + n_j*self.alpha)\n",
    "                    \n",
    "                    # Adding to likelihood tensor,\n",
    "                    self.likelihoods[cls_idx, feature_idx, feature_value] = Pxiy\n",
    "\n",
    "    def predict(self, X_sample):\n",
    "\n",
    "        # Initialising arrays,\n",
    "        self.probs = []\n",
    "        self.PXy_vector = []\n",
    "\n",
    "        for class_idx, cls in enumerate(self.y_possible_values):\n",
    "\n",
    "            # Extracting required P(y=y_l),\n",
    "            Py = self.Py_vector[class_idx]\n",
    "\n",
    "            # Computing P(X|y=y_l),\n",
    "            PXy = 1 # <-- Placeholder value\n",
    "\n",
    "            for feature_idx, feature in enumerate(X_sample):\n",
    "                PXiy = self.likelihoods[class_idx][feature_idx][feature] # <-- Extracted from pre-computed matrix\n",
    "                PXy = PXy*PXiy\n",
    "            self.PXy_vector.append(PXy)\n",
    "\n",
    "        # Computing P(X) from the vectors (normalisation),\n",
    "        PX = np.dot(self.PXy_vector, self.Py_vector)\n",
    "\n",
    "        # Final calculation,\n",
    "        self.probs = (self.Py_vector*self.PXy_vector)/PX\n",
    "\n",
    "        # Prediction as the most likely probability,\n",
    "        pred = self.y_possible_values[np.argmax(self.probs)]\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"The model prediction is taken as the class with the most likely probability.\"\"\"\n",
    "\n",
    "        # Counting number of correct predictions,\n",
    "        correct = 0\n",
    "        for i, X_sample in enumerate(X):\n",
    "            pred = self.predict(X_sample)\n",
    "            if pred == y[i]:\n",
    "                correct += 1\n",
    "\n",
    "        # Computing accuracy,\n",
    "        accuracy = correct/len(y)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67c2bb",
   "metadata": {},
   "source": [
    "Our model has an accuracy of roughly around 92-95% on the testing dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa531764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9551971326164874"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HYPERPARAMETERS,\n",
    "ALPHA = 0.1\n",
    "\n",
    "# Creating data split,\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Creating and fitting model on training data,\n",
    "clf = CategoricalNaiveBayes(alpha=ALPHA)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Computing accuracy on testing data,\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1778840",
   "metadata": {},
   "source": [
    "When benchmarking our model in comparision to scikit-learn's implementation of the categorical naive Bayes model, the results are comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c667e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9551971326164874"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and fitting model on training data,\n",
    "clf = CategoricalNB(alpha=ALPHA)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Computing accuracy on testing data,\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8915",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "However, there are differences between our implementation and that of scikit-learn's. Firstly, to save computational resources, $Z = P(X)$ is not calculated since it is just a normalisation constant. Moroever, our implementation allows for the possibility of floating-point underflow when calculating $P(X|y)$ since $P(x_1|y)P(x_2|y)P(x_3|y)...$ may be an extremely small value. To prevent this, the logarithmic probabilities $\\log{[P(y|X)]}$ are calculated instead. Mathematically,\n",
    "\n",
    "$$P(y=y_l|X) = \\frac{1}{Z}P(y=y_l)\\prod_{j=1}^{M} P(x_j|y=y_k)$$\n",
    "\n",
    "$$P(y=y_l|X) \\sim P(y=y_l)\\prod_{j=1}^{M} P(x_j|y=y_k)$$\n",
    "\n",
    "$$\\log{[P(y=y_l|X)]} \\sim \\log{[P(y=y_l)]} + \\sum_{j=1}^{M} \\log{[P(x_j|y=y_k)]}$$\n",
    "\n",
    "In our array formalism, we have, \n",
    "\n",
    "$$\\log{(\\mathbf{P})} \\sim \\log{(\\mathbf{P_y})} + \\log{(\\mathbf{P_{Xy}})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32460b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNaiveBayes():\n",
    "    \"\"\"\n",
    "    A simple implementation of the Categorical Naive Bayes classifier for discrete/categorical feature data.\n",
    "\n",
    "    This model assumes that each feature follows a multinomial distribution conditioned on the class label.\n",
    "    It applies Laplace smoothing controlled by the hyperparameter `alpha`.\n",
    "\n",
    "    Attributes:\n",
    "        X (ndarray): The training feature matrix of shape (n_samples, n_features).\n",
    "        y (ndarray): The target labels corresponding to `X`, of shape (n_samples,).\n",
    "        alpha (float): Laplace smoothing parameter (must be >= 0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"Constructor method. Class variables are stored or computed.\"\"\"\n",
    "\n",
    "        # Creating class variables,\n",
    "        self.X, self.y = None, None\n",
    "        self.nX_possible_values, self.y_possible_values = None, None\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Placeholders,\n",
    "        self.Py_vector = None\n",
    "        self.likelihoods = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Bulk of the required calculation is performed by fitting the model. Specifically, our probability vectors are \n",
    "        computed.\"\"\"\n",
    "\n",
    "        # Assigning class variables,\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "        # Computing possible values for features and classes,\n",
    "        self.nX_possible_values = np.apply_along_axis(lambda col: len(np.unique(col)), axis=0, arr=X)\n",
    "        self.n_features = X.shape[1]\n",
    "        self.feature_values = [np.unique(X[:, i]) for i in range(self.n_features)]\n",
    "\n",
    "        # Computing P(y) for all class labels,\n",
    "        self.y_possible_values, y_counts = np.unique(y, return_counts=True)\n",
    "        self.Py_vector = y_counts/len(y) # <-- Formula \n",
    "        self.n_classes = len(self.y_possible_values)\n",
    "        self.max_categories = max(self.nX_possible_values)\n",
    "\n",
    "        # Creating likelihood tensor,\n",
    "        self.likelihoods = np.full((self.n_classes, self.n_features, self.max_categories), fill_value=1, dtype=float)\n",
    "\n",
    "        # Computing all P(x_i|y=yk) for lik,\n",
    "        for cls_idx, cls in enumerate(self.y_possible_values):\n",
    "\n",
    "            cls_idxs = np.where(self.y==cls)[0]\n",
    "            X_given_y = X[cls_idxs]\n",
    "\n",
    "            # Double loop over features and then possible feature values,\n",
    "            for feature_idx in range(self.n_features):\n",
    "\n",
    "                # P(x_i|y=yk) for all values x_i can take,\n",
    "                for feature_value in self.feature_values[feature_idx]:\n",
    "                    n_instances = len(np.where(X_given_y.T[feature_idx]==feature_value)[0])\n",
    "                    n_j = self.nX_possible_values[feature_idx]\n",
    "                    Pxiy = (n_instances + self.alpha)/(len(cls_idxs) + n_j*self.alpha)\n",
    "                    \n",
    "                    # Adding to likelihood tensor,\n",
    "                    self.likelihoods[cls_idx, feature_idx, feature_value] = Pxiy\n",
    "\n",
    "    def predict(self, X_sample, return_probs=False):\n",
    "\n",
    "        # Initialising arrays,\n",
    "        self.Log_probs = []\n",
    "        self.probs = None\n",
    "\n",
    "        for class_idx, cls in enumerate(self.y_possible_values):\n",
    "\n",
    "            # Extracting required P(y=y_l),\n",
    "            Log_Py = np.log(self.Py_vector[class_idx])\n",
    "\n",
    "            # Computing P(X|y=y_l),\n",
    "            Log_PXy = 0 # <-- Placeholder value\n",
    "\n",
    "            for feature_idx, feature_value in enumerate(X_sample):\n",
    "                Log_PXiy = np.log(self.likelihoods[class_idx][feature_idx][feature_value]) # <-- Extracted from pre-computed matrix\n",
    "                Log_PXy += Log_PXiy\n",
    "\n",
    "            # Final calculation,\n",
    "            Log_PykX = Log_Py + Log_PXy\n",
    "            self.Log_probs.append(Log_PykX)\n",
    "\n",
    "        if return_probs:\n",
    "\n",
    "            # Recovering PXy_vector,\n",
    "            PXy_vector = np.exp(self.Log_probs)\n",
    "            \n",
    "            # Computing P(X),\n",
    "            PX = np.dot(self.Py_vector, PXy_vector)\n",
    "\n",
    "            # Calculating probabilities, \n",
    "            self.probs = (self.Py_vector*PXy_vector)/PX\n",
    "\n",
    "            return self.probs\n",
    "        else:\n",
    "            # Prediction as the most likely probability,\n",
    "            pred = self.y_possible_values[np.argmax(self.Log_probs)]\n",
    "\n",
    "            return pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"The model prediction is taken as the class with the most likely probability.\"\"\"\n",
    "\n",
    "        # Counting number of correct predictions,\n",
    "        correct = 0\n",
    "        for i, X_sample in enumerate(X):\n",
    "            pred = self.predict(X_sample)\n",
    "            if pred == y[i]:\n",
    "                correct += 1\n",
    "\n",
    "        # Computing accuracy,\n",
    "        accuracy = correct/len(y)\n",
    "\n",
    "        return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
