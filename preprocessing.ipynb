{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55578b8",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff26f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSL imports,\n",
    "import collections\n",
    "import re\n",
    "\n",
    "# Third-party imports,\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c932ab",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "Dataset taken from: https://www.kaggle.com/code/zabihullah18/email-spam-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729ba8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms_text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sms_text  spam\n",
       "0  Go until jurong point, crazy.. Available only ...     0\n",
       "1                      Ok lar... Joking wif u oni...     0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3  U dun say so early hor... U c already then say...     0\n",
       "4  Nah I don't think he goes to usf, he lives aro...     0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...     1\n",
       "6  Even my brother is not like to speak with me. ...     0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...     0\n",
       "8  WINNER!! As a valued network customer you have...     1\n",
       "9  Had your mobile 11 months or more? U R entitle...     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data, \n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Dropping columns not needed,\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "\n",
    "# Chaning columns names,\n",
    "df = df.rename(columns={\"v2\":\"sms_text\", \"v1\":\"spam\"})\n",
    "\n",
    "# Remapping,\n",
    "df[\"spam\"] = df[\"spam\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# Switching columns,\n",
    "df = df.iloc[:, [1, 0]]\n",
    "\n",
    "# Display dataframe,\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828f52c",
   "metadata": {},
   "source": [
    "# Word Analysis\n",
    "\n",
    "We want to find the most common words that are in spam SMS texts. First, we define the function for counting the frequency of words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdf4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string_obj):\n",
    "    \"\"\"Counts the normalised frequency of words in a string and returns them as a dictionary.\"\"\"\n",
    "\n",
    "    # Cleaning text,\n",
    "    string_obj = re.sub(r\"[^\\w\\s']\", \"\", string_obj.lower().strip())\n",
    "\n",
    "    # Creating list of words,\n",
    "    word_list = string_obj.split()\n",
    "    word_dict = {}\n",
    "\n",
    "    # Counting total words,\n",
    "    n_words = len(word_list)\n",
    "\n",
    "    for word in word_list:\n",
    "        if word in word_dict:\n",
    "            # Incrementing (normalised) count if word exists,\n",
    "            word_dict[word] += 1/n_words\n",
    "        else:\n",
    "            # Initialising (normalised) count if word does not exist in dictionary,\n",
    "            word_dict[word] = 1/n_words\n",
    "\n",
    "    # Sorting dictionary in descending order,\n",
    "    word_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9439ca1",
   "metadata": {},
   "source": [
    "We want to consider words which are much more likely to show up in spam SMS texts compared to regular texts, but are also not rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a903b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['call', 0.016528331946966957], ['to', 0.016033022403738204], ['free', 0.009975193739621474], ['txt', 0.008634706474211041], ['your', 0.00855729722424854], ['or', 0.007631647755458866], ['now', 0.00702437964389313], ['mobile', 0.006944628926538471], ['claim', 0.006633499170812593], ['text', 0.0057966940252954825], ['stop', 0.005522304840908529], ['2', 0.005463448950449189], ['reply', 0.00534462182740462], ['from', 0.005148385454397471], ['prize', 0.005093579720445386], ['4', 0.004448466961497021], ['won', 0.004323619995261782], ['our', 0.0038463587155233322], ['ur', 0.0038360389843392468], ['nokia', 0.0038180958709486184], ['cash', 0.0035294532150517855], ['contact', 0.003126533055589858], ['guaranteed', 0.002961383558398482], ['service', 0.002957205719627638], ['new', 0.0029103895155496795], ['win', 0.0028620974547194176], ['tone', 0.0027837005448945735], ['customer', 0.002451681595314475], ['per', 0.0023999496240895203], ['chat', 0.002329048414207692], ['awarded', 0.0022506515043828478], ['with', 0.0022234983688928693], ['draw', 0.002187245994444034], ['책1000', 0.002072968490878939], ['week', 0.0020461815030565276], ['who', 0.001982775993117714], ['latest', 0.0019661866872568595], ['line', 0.0019620088484860153], ['send', 0.0019518111665244638], ['receive', 0.0018911076386041864], ['18', 0.0018360578062070608], ['책2000', 0.0018360578062070608], ['mins', 0.0018277021286653727], ['landline', 0.001804355051237654], ['shows', 0.0017726522962682471], ['camera', 0.0017609787575543878], ['16', 0.0017609787575543878], ['box', 0.0017451273800696842], ['only', 0.0017424253980511486], ['holiday', 0.001720920325043293]]\n"
     ]
    }
   ],
   "source": [
    "# Construcing spam and ham dataframes,\n",
    "df_spam = df[df[\"spam\"] == 1]\n",
    "df_ham = df[df[\"spam\"] == 0]\n",
    "\n",
    "# Extracting all spam and ham texts into a string,\n",
    "spam_string = \"\"\n",
    "ham_string = \"\"\n",
    "\n",
    "for text in df_spam[\"sms_text\"]:\n",
    "    spam_string += text\n",
    "\n",
    "for text in df_ham[\"sms_text\"]:\n",
    "    ham_string += text\n",
    "\n",
    "# Counting words,\n",
    "spam_words = count_words(spam_string)\n",
    "ham_words = count_words(ham_string)\n",
    "\n",
    "# PARAMETERS,\n",
    "THRESHOLD = 1.5\n",
    "\n",
    "\"\"\"Finding the top words which have the biggest difference in probability of appearing in spam texts compared\n",
    "to those of regular ones.\"\"\"\n",
    "\n",
    "spammy_words = []\n",
    "for spam_word in spam_words:\n",
    "    if spam_words[spam_word] > THRESHOLD * ham_words.get(spam_word, 0):\n",
    "        prob_diff = spam_words[spam_word] - ham_words.get(spam_word, 0)\n",
    "        spammy_words.append([spam_word, prob_diff])\n",
    "\n",
    "spammy_words = sorted(spammy_words, key=lambda x: x[1], reverse=True)\n",
    "print(spammy_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c54bc",
   "metadata": {},
   "source": [
    "The top 50 words which appear in SMS spam texts are considered. Now we construct a vector for each text in the dataset. Each vector has a feature length of the number of words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1e91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(string_obj, features):\n",
    "    \"\"\"Creates a binary feature vector given a SMS text and a list of feature names.\"\"\"\n",
    "\n",
    "    feature_vector = []\n",
    "    for feature in features:\n",
    "\n",
    "        # Spammy word not found,\n",
    "        if string_obj.count(feature) == 0:\n",
    "            feature_vector.append(0)\n",
    "        \n",
    "        # Spammy word found at least once,\n",
    "        else:\n",
    "            feature_vector.append(1)\n",
    "\n",
    "    # Returns a list,\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86dac5",
   "metadata": {},
   "source": [
    "Creating our new dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad3aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['call', 'to', 'free', 'txt', 'your', 'or', 'now', 'mobile', 'claim', 'text', 'stop', '2',\n",
    " 'reply', 'from', 'prize', '4', 'won', 'our', 'ur', 'nokia', 'cash', 'contact', 'guaranteed',\n",
    " 'service', 'new', 'win', 'tone', 'customer', 'per', 'chat', 'awarded', 'with', 'draw', '책1000',\n",
    " 'week', 'who', 'latest', 'line', 'send', 'receive', '18', '책2000', 'mins', 'landline', 'shows',\n",
    " 'camera', '16', 'box', 'only', 'holiday']\n",
    "\n",
    "# Creating feature vectors,\n",
    "feature_vectors = []\n",
    "for text in df[\"sms_text\"]:\n",
    "    feature_vector = create_feature_vector(text, FEATURES)\n",
    "    feature_vectors.append(feature_vector)\n",
    "\n",
    "feature_vectors = np.array(feature_vectors)\n",
    "\n",
    "# Creating target vector,\n",
    "targets = df[\"spam\"].to_numpy()\n",
    "\n",
    "# Saving,\n",
    "np.savez(\"dataset.npz\", features=feature_vectors, targets=targets, feature_labels=FEATURES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
